# Rate Limiter Global API 42 avec File d'Attente

## ğŸ¯ Objectif

Garantir **200ms minimum** entre **TOUTES** les requÃªtes vers l'API 42, peu importe le nombre d'utilisateurs simultanÃ©s, en utilisant une **vraie file d'attente thread-safe**.

## ğŸ—ï¸ Architecture

### Classe `API42RateLimiter`

```typescript
class API42RateLimiter {
  private queue: QueuedRequest<any>[] = [];      // File d'attente des requÃªtes
  private processing = false;                     // Flag de traitement en cours
  private lastRequestTime = 0;                    // Timestamp de la derniÃ¨re requÃªte
  private readonly minDelay = 200;                // DÃ©lai minimum (ms)
}
```

### Composants

1. **File d'attente (queue)** : Stocke toutes les requÃªtes en attente
2. **Mutex virtuel (processing)** : EmpÃªche le traitement concurrent
3. **Horodatage (lastRequestTime)** : Calcule le temps Ã©coulÃ©
4. **Wrapper `api42Request()`** : Interface simple pour les routes

## ğŸ”„ Flux de Fonctionnement

### ScÃ©nario 1 : RequÃªte unique (user seul)

```
User A lance 3 requÃªtes (projects, cursus, events)
â”‚
â”œâ”€> Request 1 (projects)
â”‚   â”œâ”€> AjoutÃ©e Ã  la queue [1]
â”‚   â”œâ”€> Processing dÃ©marre
â”‚   â”œâ”€> ExÃ©cution immÃ©diate (pas de dÃ©lai car premiÃ¨re requÃªte)
â”‚   â””â”€> ComplÃ©tÃ©e (lastRequestTime = T0)
â”‚
â”œâ”€> Request 2 (cursus)
â”‚   â”œâ”€> AjoutÃ©e Ã  la queue [1]
â”‚   â”œâ”€> Attend 200ms (T0 + 200ms - now)
â”‚   â””â”€> ComplÃ©tÃ©e (lastRequestTime = T0 + 200ms)
â”‚
â””â”€> Request 3 (events)
    â”œâ”€> AjoutÃ©e Ã  la queue [1]
    â”œâ”€> Attend 200ms
    â””â”€> ComplÃ©tÃ©e (lastRequestTime = T0 + 400ms)

Total: ~400ms pour 3 requÃªtes
```

### ScÃ©nario 2 : Utilisateurs multiples (concurrence)

```
T=0ms    User A lance Request 1 (projects)
         â”‚
         â”œâ”€> Queue: [A1] â†’ Processing dÃ©marre
         â””â”€> A1 exÃ©cutÃ©e immÃ©diatement

T=50ms   User B lance Request 1 (projects)
         â”‚
         â”œâ”€> Queue: [B1] â†’ Attente (processing=true)
         â””â”€> A1 toujours en cours

T=100ms  User A lance Request 2 (cursus)
         â”‚
         â””â”€> Queue: [B1, A2] â†’ Attente

T=150ms  A1 complÃ©tÃ©e (lastRequestTime = 150ms)
         â”‚
         â””â”€> Queue traite B1 automatiquement

T=150ms  B1 vÃ©rifie dÃ©lai
         â”‚
         â”œâ”€> Temps Ã©coulÃ©: 0ms (150 - 150)
         â”œâ”€> Attente nÃ©cessaire: 200ms
         â””â”€> Sleep 200ms

T=350ms  B1 exÃ©cutÃ©e (lastRequestTime = 350ms)
         â”‚
         â””â”€> Queue traite A2 automatiquement

T=350ms  A2 vÃ©rifie dÃ©lai
         â”‚
         â”œâ”€> Temps Ã©coulÃ©: 0ms
         â”œâ”€> Attente nÃ©cessaire: 200ms
         â””â”€> Sleep 200ms

T=550ms  A2 exÃ©cutÃ©e (lastRequestTime = 550ms)
         â”‚
         â””â”€> Queue vide, processing = false

âœ… RÃ©sultat: 200ms garantis entre chaque requÃªte
```

### ScÃ©nario 3 : Burst simultanÃ© (3 users en mÃªme temps)

```
T=0ms    User A, B, C lancent simultanÃ©ment leurs requÃªtes
         â”‚
         â”œâ”€> Queue: [A1, A2, A3, B1, B2, B3, C1, C2, C3]
         â””â”€> Processing dÃ©marre

T=0ms    A1 exÃ©cutÃ©e immÃ©diatement
T=200ms  A2 exÃ©cutÃ©e (wait 200ms)
T=400ms  A3 exÃ©cutÃ©e (wait 200ms)
T=600ms  B1 exÃ©cutÃ©e (wait 200ms)
T=800ms  B2 exÃ©cutÃ©e (wait 200ms)
T=1000ms B3 exÃ©cutÃ©e (wait 200ms)
T=1200ms C1 exÃ©cutÃ©e (wait 200ms)
T=1400ms C2 exÃ©cutÃ©e (wait 200ms)
T=1600ms C3 exÃ©cutÃ©e (wait 200ms)

Total: 1.6s pour traiter 3 utilisateurs
Queue max: 9 requÃªtes
âœ… Aucun spam API 42, dÃ©lai garanti
```

## ğŸ” Code DÃ©tail

### Ajout Ã  la queue (`enqueue`)

```typescript
async enqueue<T>(execute: () => Promise<T>): Promise<T> {
  return new Promise((resolve, reject) => {
    // 1. Ajouter la requÃªte Ã  la queue
    this.queue.push({ execute, resolve, reject });
    
    // 2. Logger la taille
    console.log(`[API42 Queue] Request added (size: ${this.queue.length})`);
    
    // 3. DÃ©marrer le traitement si pas dÃ©jÃ  en cours
    if (!this.processing) {
      this.processQueue();
    }
  });
}
```

**ClÃ©** : La Promise est rÃ©solue **plus tard** par `processQueue()`, pas immÃ©diatement.

### Traitement de la queue (`processQueue`)

```typescript
private async processQueue(): Promise<void> {
  // 1. Protection contre le traitement concurrent
  if (this.processing || this.queue.length === 0) return;
  
  this.processing = true; // ğŸ”’ Mutex lock

  while (this.queue.length > 0) {
    const request = this.queue.shift()!; // Retirer de la queue
    
    // 2. Calculer le dÃ©lai nÃ©cessaire
    const timeSinceLastRequest = Date.now() - this.lastRequestTime;
    
    if (timeSinceLastRequest < this.minDelay) {
      const waitTime = this.minDelay - timeSinceLastRequest;
      await new Promise(resolve => setTimeout(resolve, waitTime)); // â±ï¸ Attente
    }

    // 3. ExÃ©cuter la requÃªte
    try {
      const result = await request.execute();
      this.lastRequestTime = Date.now(); // ğŸ“ Mise Ã  jour timestamp
      request.resolve(result); // âœ… RÃ©solution de la Promise originale
    } catch (error) {
      request.reject(error); // âŒ Rejet de la Promise
    }
  }

  this.processing = false; // ğŸ”“ Mutex unlock
}
```

**Points clÃ©s** :
- `processing = true` empÃªche le traitement concurrent
- Boucle `while` traite toute la queue
- Chaque requÃªte attend exactement 200ms aprÃ¨s la prÃ©cÃ©dente
- Les Promises sont rÃ©solues dans l'ordre FIFO

### Wrapper d'utilisation

```typescript
async function api42Request<T>(url: string, token: string): Promise<T> {
  return api42RateLimiter.enqueue(async () => {
    const response = await axios.get(url, {
      headers: { Authorization: `Bearer ${token}` },
    });
    return response.data;
  });
}
```

**Utilisation dans les routes** :
```typescript
// Au lieu de:
const data = await axios.get(url, { headers: { Authorization: token } });

// Maintenant:
const data = await api42Request(url, token);
```

## ğŸ“Š Avantages

### âœ… Thread-safe (ou plutÃ´t async-safe)
- Un seul traitement Ã  la fois grÃ¢ce au flag `processing`
- Pas de race condition possible

### âœ… FIFO garanti
- Les requÃªtes sont traitÃ©es dans l'ordre d'arrivÃ©e
- `queue.shift()` retire toujours le premier Ã©lÃ©ment

### âœ… DÃ©lai garanti
- 200ms **minimum** entre chaque requÃªte
- Valide mÃªme avec 100 users simultanÃ©s

### âœ… Transparent
- Aucun changement dans la logique des routes
- Juste remplacer `axios.get()` par `api42Request()`

### âœ… Observable
- Logs clairs de la queue (`[API42 Queue]`)
- Taille de la queue affichÃ©e en temps rÃ©el

## ğŸ“ Logs Exemple

```
[API42 Queue] Request added to queue (queue size: 1)
[API42 Queue] Executing request (0 remaining in queue)
[API42 Routes] âœ“ Projects fetched: 30 items

[API42 Queue] Request added to queue (queue size: 1)
[API42 Queue] Waiting 150ms before next request (0 in queue)...
[API42 Queue] Executing request (0 remaining in queue)
[API42 Routes] âœ“ Cursus fetched: 3 items

[API42 Queue] Request added to queue (queue size: 1)
[API42 Queue] Waiting 200ms before next request (0 in queue)...
[API42 Queue] Executing request (0 remaining in queue)
[API42 Routes] âœ“ Events fetched: 20 items

[API42 Queue] Queue empty, processing stopped
```

## âš¡ Performance

### Temps thÃ©orique

Pour N requÃªtes :
- Temps minimum = N * 200ms
- Temps maximum = N * 200ms + temps API

**Exemple** :
- 3 requÃªtes (user-data) : 400ms + latence API (~1.4s total)
- 9 requÃªtes (3 users) : 1600ms + latence API (~3s total)

### Comparaison

| ScÃ©nario | Sans queue | Avec queue |
|----------|------------|------------|
| 1 user, 3 req | 3 req simultanÃ©es = SPAM | 3 req espacÃ©es de 200ms |
| 3 users simultanÃ©s | 9 req simultanÃ©es = RATE LIMIT | 9 req sÃ©quentielles |
| 10 users burst | 30 req = BAN | 30 req sur 6s = OK |

## ğŸ›¡ï¸ Protection Rate Limit

L'API 42 a typiquement une limite de **~1200 req/h** (environ 20 req/min).

**Sans queue** :
- 10 users chargent la page en mÃªme temps
- 10 Ã— 3 = 30 requÃªtes instantanÃ©es
- 30 req / 20 max = **RATE LIMIT** ğŸ”´

**Avec queue** :
- 10 users chargent la page
- 30 requÃªtes espacÃ©es de 200ms
- 30 Ã— 200ms = 6 secondes
- 30 req / 6s = 5 req/s = 300 req/min
- **Toujours dans la limite** âœ…

## ğŸ”§ Configuration

Pour ajuster le dÃ©lai :

```typescript
class API42RateLimiter {
  private readonly minDelay = 200; // â† Changer ici
}
```

**Recommandations** :
- 200ms = bon Ã©quilibre (rapide + sÃ»r)
- 100ms = plus rapide mais risquÃ©
- 500ms = trÃ¨s sÃ»r mais lent

## ğŸ“ Maintenance

### Vider la queue (redÃ©marrage serveur)
```bash
# La queue est en mÃ©moire, elle disparaÃ®t au restart
npm run dev
```

### Monitoring en production
```typescript
// Ajouter un endpoint de monitoring
fastify.get('/api/queue-status', () => ({
  queueSize: api42RateLimiter.queue.length,
  processing: api42RateLimiter.processing,
  lastRequest: api42RateLimiter.lastRequestTime,
}));
```

## ğŸ¯ Conclusion

Ce systÃ¨me garantit une utilisation **respectueuse et efficace** de l'API 42, mÃªme sous forte charge. La file d'attente assure qu'aucune requÃªte ne sera jamais "oubliÃ©e" ou rejetÃ©e pour cause de rate limit.
